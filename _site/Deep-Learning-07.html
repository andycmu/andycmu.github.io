<!DOCTYPE html>
<html>

<head>
	<!-- Meta -->
	<meta charset="UTF-8"/>
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
	<meta name="generator" content="Jekyll">

	<title>Deep Learning Concept 7: softmax</title>
  <meta name="description" content="">

	<!-- CSS & fonts -->
	<link rel="stylesheet" href="/css/main.css">

	<!-- RSS -->
	<link href="/atom.xml" type="application/atom+xml" rel="alternate" title="ATOM Feed" />

  <!-- Favicon -->
  <link rel="shortcut icon" type="image/png" href="img/favicon.png">

</head>


<body>
	<div id="wrap">
	  	
	  	<!-- Navigation -->
	  	<nav id="nav">
	<div id="nav-list">
		<a href="//">Home</a>

		<!-- Nav pages -->
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
    
    <!-- Nav links -->
	  


	</div>
  
  <!-- Nav footer -->
	
	  <footer>
	
	<span>version 1.0.0</span>

</footer>
	

</nav>

    
    <!-- Icon menu -->
	  <a id="nav-menu">
	  	<div id="menu"></div>
	  </a>

      <!-- Header -->
      
        <header id="header" class="parent justify-spaceBetween">
  <div class="inner w100 relative">
    <span class="f-left">  
      <a href="//">
        <h1>
          <span>Xinyu Wu</span>
        </h1>
      </a>
    </span>
    <span id="nav-links" class="absolute right bottom">
      <!-- Nav pages -->
	    
	      
	    
	      
	    
	      
	    
	      
	    
	      
	    
	      
	    
      
      <!-- Nav links -->
	    


    </span>
  </div>
</header>




      

    <!-- Main content -->
	  <div id="container">
		  
		<main>

			<article id="post-page">
	<h2>Deep Learning Concept 7: softmax</h2>		
	<time datetime="2018-09-11T00:00:00-04:00" class="by-line">11 Sep 2018</time>
	<div class="content">

		<h3 id="why-we-need-softmax">Why we need softmax?</h3>
<p>In binary classification problem, we can use logistic regression to do the classification, namely compute the probability of each class. In multi-class classification, we need to have a similar way to convert the neural network output to the probability for each class. Softmax helps us with this.</p>

<h3 id="what-is-softmax">What is softmax?</h3>
<p>Softmax is an activation function like sigmoid and relu. It applies non-linearity to the last layer and outputs a probability for each class. The sum of the output vector value <strong>from</strong> softmax layer will be 1. Simple python implementation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">logits</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">exps</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">logits</span><span class="p">]</span>
<span class="n">sum_of_exps</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">exps</span><span class="p">)</span>
<span class="n">softmax</span> <span class="o">=</span> <span class="p">[</span><span class="n">j</span><span class="o">/</span><span class="n">sum_of_exps</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">exps</span><span class="p">]</span>
</code></pre></div></div>

<h3 id="how-to-train-with-softmax">How to train with softmax</h3>
<p>Since softmax helps you convert the output to a vector of probability for each class, you can calculate the <strong><a href="https://andycmu.github.io/Deep-Learning-06">cross-entrop loss</a></strong> using softmax output. Then to calculate the cost J of the entire training set, you average the loss of each training example. At last, we can calculate the gradient descent of cost J and back prop the value.</p>

		
	</div>
</article>



	  </main>
		
		  <!-- Pagination links -->
      

	  </div>
	    
	    <!-- Footer -->
	    <footer><span></span></footer>


	    <!-- Script -->
      <script src="/js/main.js"></script>	
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


	</div>
</body>
</html>
