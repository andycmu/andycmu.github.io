<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Xinyu</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <id>http://localhost:4000</id>
 <updated>2018-09-08T11:16:35-04:00</updated>
 <author>
   <name></name>
   <uri></uri>
   <email></email>
 </author>

 

 <entry>
   <title>Deep Learning Concept 4: Mini Batch</title>
   <link href="http://localhost:4000/Deep-Learning-04"/>
   <id>http://localhost:4000/Deep-Learning-04</id>
   <updated>2018-09-08T00:00:00-04:00</updated>
   <content type="html">&lt;h3 id=&quot;what-is-mini-batch&quot;&gt;What is mini batch&lt;/h3&gt; &lt;p&gt;To understand what is mini batch, letâ€™s first take a look at &lt;strong&gt;batch gradient descent&lt;/strong&gt;. For batch gradient descent, in each iteration you go through all the training data. You expect the cost to go down on &lt;strong&gt;every single&lt;/strong&gt; iteration.&lt;/p&gt; &lt;p&gt;In &lt;strong&gt;mini batch gradient descent&lt;/strong&gt;,...</content>
 </entry>

 

 <entry>
   <title>Deep Learning Concept 3: Learning Rate</title>
   <link href="http://localhost:4000/Deep-Learning-03"/>
   <id>http://localhost:4000/Deep-Learning-03</id>
   <updated>2018-09-05T00:00:00-04:00</updated>
   <content type="html">&lt;h3 id=&quot;what-is-a-learning-rate&quot;&gt;What is a learning rate?&lt;/h3&gt; &lt;p&gt;Learning rate is a hyper-parameter that controls how much we are adjusting the weights of our network with respect to the loss gradient&lt;/p&gt; &lt;script type=&quot;math/tex; mode=display&quot;&gt;new\_weight = existing\_weight - learning\_rate * gradient&lt;/script&gt; &lt;h3 id=&quot;why-is-learning-rate-important&quot;&gt;Why is learning rate important?&lt;/h3&gt; &lt;p&gt;Learning rate affects how quickly our...</content>
 </entry>

 

 <entry>
   <title>Deep Learning Concept 2: mAP on object detection</title>
   <link href="http://localhost:4000/Deep-Learning-02"/>
   <id>http://localhost:4000/Deep-Learning-02</id>
   <updated>2018-09-03T00:00:00-04:00</updated>
   <content type="html">&lt;p&gt;Object detection model evaluation has two parts: what object it is (classification) and where it is (localization).&lt;/p&gt; &lt;p&gt;To evaluate the classification part, there are two useful metrics: precision and recall. They are defined as&lt;/p&gt; &lt;script type=&quot;math/tex; mode=display&quot;&gt;Precision = \frac{true_positive}{true_positive + false_positive}&lt;/script&gt; &lt;script type=&quot;math/tex; mode=display&quot;&gt;Recall = \frac{true_positive}{true_positive + false_negative}&lt;/script&gt; &lt;p&gt;Precision says...</content>
 </entry>

 

 <entry>
   <title>Deep Learning Concept 1: non-maximum suppression</title>
   <link href="http://localhost:4000/Deep_Learning-01"/>
   <id>http://localhost:4000/Deep_Learning-01</id>
   <updated>2018-08-27T00:00:00-04:00</updated>
   <content type="html">&lt;h3 id=&quot;problem-it-tries-to-solve&quot;&gt;Problem it tries to solve&lt;/h3&gt; &lt;p&gt;object detection algorithm may find multiple objects of the same objects. Non-maximum suppression makes sure that the algorithm only output one detection per objects.&lt;/p&gt; &lt;h3 id=&quot;how-it-works&quot;&gt;How it works&lt;/h3&gt; &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Discard all boxes with p &amp;lt; threshold (e.g. 0.6) While (remaining box number...</content>
 </entry>

 

 <entry>
   <title>Deep Learning Concept Series</title>
   <link href="http://localhost:4000/Deep-Learning-Start"/>
   <id>http://localhost:4000/Deep-Learning-Start</id>
   <updated>2018-08-26T00:00:00-04:00</updated>
   <content type="html">&lt;p&gt;There are lots of lectures, blogs, notes about deep learning. This series aims to explain some fundamental concept. The goal is to keep it as simple and short as possible. It will try to resolve two questions: what problem it is trying to solve and how it works.&lt;/p&gt;
</content>
 </entry>

 

</feed>