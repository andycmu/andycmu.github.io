<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Xinyu</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <id>http://localhost:4000</id>
 <updated>2018-09-11T22:47:13-04:00</updated>
 <author>
   <name></name>
   <uri></uri>
   <email></email>
 </author>

 

 <entry>
   <title>Deep Learning Concept 7: softmax</title>
   <link href="http://localhost:4000/Deep-Learning-07"/>
   <id>http://localhost:4000/Deep-Learning-07</id>
   <updated>2018-09-11T00:00:00-04:00</updated>
   <content type="html">&lt;h3 id=&quot;why-we-need-softmax&quot;&gt;Why we need softmax?&lt;/h3&gt; &lt;p&gt;In binary classification problem, we can use logistic regression to do the classification, namely compute the probability of each class. In multi-class classification, we need to have a similar way to convert the neural network output to the probability for each class. Softmax helps us...</content>
 </entry>

 

 <entry>
   <title>Deep Learning Concept 6: cross entropy loss</title>
   <link href="http://localhost:4000/Deep-Learning-06"/>
   <id>http://localhost:4000/Deep-Learning-06</id>
   <updated>2018-09-10T00:00:00-04:00</updated>
   <content type="html">&lt;p&gt;The most common loss function used in machine learning are MAE (mean absolute error), MSE (mean squared error) and cross entropy loss. This blog introduces cross entropy loss.&lt;br /&gt; We calcualte this loss for classification model which outputs a probability for each class.&lt;/p&gt; &lt;h3 id=&quot;explanation-with-intuition&quot;&gt;Explanation with intuition&lt;/h3&gt; &lt;p&gt;this loss increases...</content>
 </entry>

 

 <entry>
   <title>Deep Learning Concept 5: channel</title>
   <link href="http://localhost:4000/Deep-Learning-05"/>
   <id>http://localhost:4000/Deep-Learning-05</id>
   <updated>2018-09-09T00:00:00-04:00</updated>
   <content type="html">&lt;p&gt;In each one of the convolutional layer, there are input and output images. Each images will have &lt;script type=&quot;math/tex&quot;&gt;W \times H&lt;/script&gt; 2D size and a third dimension called channel. What does channel mean here?&lt;/p&gt; &lt;p&gt;In the simple case, we have the initial input image as RGB image. Then there are...</content>
 </entry>

 

 <entry>
   <title>Deep Learning Concept 4: Mini Batch</title>
   <link href="http://localhost:4000/Deep-Learning-04"/>
   <id>http://localhost:4000/Deep-Learning-04</id>
   <updated>2018-09-08T00:00:00-04:00</updated>
   <content type="html">&lt;h3 id=&quot;what-is-mini-batch&quot;&gt;What is mini batch&lt;/h3&gt; &lt;p&gt;To understand what is mini batch, letâ€™s first take a look at &lt;strong&gt;batch gradient descent&lt;/strong&gt;. For batch gradient descent, in each iteration you go through all the training data. You expect the cost to go down on &lt;strong&gt;every single&lt;/strong&gt; iteration.&lt;/p&gt; &lt;p&gt;In &lt;strong&gt;mini batch gradient descent&lt;/strong&gt;,...</content>
 </entry>

 

 <entry>
   <title>Deep Learning Concept 3: Learning Rate</title>
   <link href="http://localhost:4000/Deep-Learning-03"/>
   <id>http://localhost:4000/Deep-Learning-03</id>
   <updated>2018-09-05T00:00:00-04:00</updated>
   <content type="html">&lt;h3 id=&quot;what-is-a-learning-rate&quot;&gt;What is a learning rate?&lt;/h3&gt; &lt;p&gt;Learning rate is a hyper-parameter that controls how much we are adjusting the weights of our network with respect to the loss gradient&lt;/p&gt; &lt;script type=&quot;math/tex; mode=display&quot;&gt;new\_weight = existing\_weight - learning\_rate * gradient&lt;/script&gt; &lt;h3 id=&quot;why-is-learning-rate-important&quot;&gt;Why is learning rate important?&lt;/h3&gt; &lt;p&gt;Learning rate affects how quickly our...</content>
 </entry>

 

 <entry>
   <title>Deep Learning Concept 2: mAP on object detection</title>
   <link href="http://localhost:4000/Deep-Learning-02"/>
   <id>http://localhost:4000/Deep-Learning-02</id>
   <updated>2018-09-03T00:00:00-04:00</updated>
   <content type="html">&lt;p&gt;Object detection model evaluation has two parts: what object it is (classification) and where it is (localization).&lt;/p&gt; &lt;p&gt;To evaluate the classification part, there are two useful metrics: precision and recall. They are defined as&lt;/p&gt; &lt;script type=&quot;math/tex; mode=display&quot;&gt;Precision = \frac{true_positive}{true_positive + false_positive}&lt;/script&gt; &lt;script type=&quot;math/tex; mode=display&quot;&gt;Recall = \frac{true_positive}{true_positive + false_negative}&lt;/script&gt; &lt;p&gt;Precision says...</content>
 </entry>

 

 <entry>
   <title>Deep Learning Concept 1: non-maximum suppression</title>
   <link href="http://localhost:4000/Deep_Learning-01"/>
   <id>http://localhost:4000/Deep_Learning-01</id>
   <updated>2018-08-27T00:00:00-04:00</updated>
   <content type="html">&lt;h3 id=&quot;problem-it-tries-to-solve&quot;&gt;Problem it tries to solve&lt;/h3&gt; &lt;p&gt;object detection algorithm may find multiple objects of the same objects. Non-maximum suppression makes sure that the algorithm only output one detection per objects.&lt;/p&gt; &lt;h3 id=&quot;how-it-works&quot;&gt;How it works&lt;/h3&gt; &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Discard all boxes with p &amp;lt; threshold (e.g. 0.6) While (remaining box number...</content>
 </entry>

 

 <entry>
   <title>Deep Learning Concept Series</title>
   <link href="http://localhost:4000/Deep-Learning-Start"/>
   <id>http://localhost:4000/Deep-Learning-Start</id>
   <updated>2018-08-26T00:00:00-04:00</updated>
   <content type="html">&lt;p&gt;There are lots of lectures, blogs, notes about deep learning. This series aims to explain some fundamental concept. The goal is to keep it as simple and short as possible. It will try to resolve two questions: what problem it is trying to solve and how it works.&lt;/p&gt;
</content>
 </entry>

 

</feed>