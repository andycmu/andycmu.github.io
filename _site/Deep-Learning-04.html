<!DOCTYPE html>
<html>

<head>
	<!-- Meta -->
	<meta charset="UTF-8"/>
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
	<meta name="generator" content="Jekyll">

	<title>Deep Learning Concept 4: Mini Batch</title>
  <meta name="description" content="">

	<!-- CSS & fonts -->
	<link rel="stylesheet" href="/css/main.css">

	<!-- RSS -->
	<link href="/atom.xml" type="application/atom+xml" rel="alternate" title="ATOM Feed" />

  <!-- Favicon -->
  <link rel="shortcut icon" type="image/png" href="img/favicon.png">

</head>


<body>
	<div id="wrap">
	  	
	  	<!-- Navigation -->
	  	<nav id="nav">
	<div id="nav-list">
		<a href="//">Home</a>

		<!-- Nav pages -->
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
    
    <!-- Nav links -->
	  


	</div>
  
  <!-- Nav footer -->
	
	  <footer>
	
	<span>version 1.0.0</span>

</footer>
	

</nav>

    
    <!-- Icon menu -->
	  <a id="nav-menu">
	  	<div id="menu"></div>
	  </a>

      <!-- Header -->
      
        <header id="header" class="parent justify-spaceBetween">
  <div class="inner w100 relative">
    <span class="f-left">  
      <a href="//">
        <h1>
          <span>Xinyu Wu</span>
        </h1>
      </a>
    </span>
    <span id="nav-links" class="absolute right bottom">
      <!-- Nav pages -->
	    
	      
	    
	      
	    
	      
	    
	      
	    
	      
	    
	      
	    
      
      <!-- Nav links -->
	    


    </span>
  </div>
</header>




      

    <!-- Main content -->
	  <div id="container">
		  
		<main>

			<article id="post-page">
	<h2>Deep Learning Concept 4: Mini Batch</h2>		
	<time datetime="2018-09-08T00:00:00-04:00" class="by-line">08 Sep 2018</time>
	<div class="content">

		<h3 id="what-is-mini-batch">What is mini batch</h3>
<p>To understand what is mini batch, letâ€™s first take a look at <strong>batch gradient descent</strong>. For batch gradient descent, in each iteration you go through all the training data. You expect the cost to go down on <strong>every single</strong> iteration.</p>

<p>In <strong>mini batch gradient descent</strong>, each iteration has slower batch size and it takes many iteration to finish one epoch. For each iteration, you do forward prop on training data, compute cost using loss function, compute gradient descent of cost, back prop and update weights. Therefore, you are doing all the steps we do for batch gradient descent in each mini batch iteration.</p>

<p>Two extremes of mini batch:<br />
mini batch size = M (# of training examples): Batch gradient descent; will converge steadily but take too long per iteration.<br />
minibatch size = 1 is stochastic gradient descent; will not converge, will see cost change every single training example, but lose the speed up from vectorization.</p>

<h3 id="how-do-you-choose-mini-batch-size">How do you choose mini batch size?</h3>
<p>Small training set: e.g. less than 2000, use batch gradient descent.<br />
Large training set: possibly use mini-batch size range from 64 to 512. Make sure your minibatch fit in CPU/GPU memory</p>

		
	</div>
</article>



	  </main>
		
		  <!-- Pagination links -->
      

	  </div>
	    
	    <!-- Footer -->
	    <footer><span></span></footer>


	    <!-- Script -->
      <script src="/js/main.js"></script>	
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


	</div>
</body>
</html>
